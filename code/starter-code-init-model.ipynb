{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "### Can a Classification model, trained on NLP data, correctly predict subreddit of origin between a 'good advice' and a 'bad advice' subreddit when fed `author`, `title`, and `selftext`?\n",
    "\n",
    "\n",
    "### Notes on the Data and Subreddits:\n",
    "\n",
    "**LifeProTips: (LPT)**\n",
    "- \"Tips that improve your life in one way or another\"\n",
    "- A subreddit dedicated to sharing 'helpful' user-provided advice for navigating a plethora of sitautions.\n",
    "\n",
    "**UnethicalLifeProTips: (ULPT)**\n",
    "- \"An Unethical Life Pro Tip (or ULPT) is a tip that improves your life in a meaningful way, perhaps at the    \n",
    "expense of others and/or with questionable legality. Due to their nature, do not actually follow any of these \n",
    "tipsâ€“they're just for fun. Share your best tips you've picked up throughout your life, and learn from others!\"\n",
    "- A subreddit dedicated to sharing mocking, 'joke' user-provided 'advice on a number of subjects and situations\n",
    "    \n",
    "### Predictors and Target Variable:\n",
    "\n",
    "**Model 1.0:**\n",
    "- The predictor variable is `title`.\n",
    "- The target variable is `subreddit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Function to call in Reddit info via API\n",
    "\n",
    "**Below: This function:**\n",
    "\n",
    "1) Variabilizes the 'base' url of the Pushshift API\n",
    "\n",
    "2) Uses two editable dictionaries of parameters to add-on to the API URL\n",
    "    - Current parameters are `subreddit` and `size`.\n",
    "\n",
    "3) Assign the `HTTP Reponse` request and variabilize the `status code`.\n",
    "\n",
    "4) An `if` statement checks that the value of the `status_code` for both subreddits are within the bounds of `Success 2XX`.\n",
    "\n",
    "5) If successful:\n",
    "    - The `JSON` is read-in for each subreddit, \n",
    "    - The posts are saved from each JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_json_posts(subreddit_str, size):\n",
    "    \n",
    "    # Setup URL of API\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"    \n",
    "    \n",
    "    # Create the params of the API URL\n",
    "    params = {\n",
    "        \"subreddit\": subreddit_str,\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    # Response\n",
    "    res = requests.get(base_url, params)\n",
    "    res_check = res.status_code\n",
    "    \n",
    "    # Check response is good\n",
    "    if (res_check >= 200 and res_check < 300):\n",
    "        \n",
    "        # Create JSON:\n",
    "        data = res.json()\n",
    "        posts = data[\"data\"]\n",
    "        \n",
    "        return posts\n",
    "    else:\n",
    "        return f\"Check HTTP Error: {res_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpt_posts = generate_json_posts(\"LifeProTips\", 500)\n",
    "ulpt_posts = generate_json_posts(\"UnethicalLifeProTips\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(lpt_posts, \"../datasets/lpt_posts_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Check Posts dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "\n",
      "Each post type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type: {type(lpt_posts)}\\n\")\n",
    "print(f\"Each post type: {type(lpt_posts[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cleaning:\n",
    "\n",
    "- **HTML Artifacts:**\n",
    "- **Non-Letters**\n",
    "- **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Remove Non-Letters**\n",
    "\n",
    "The function below takes in the json dictionary object and a specific key to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_non_letter(json, key):\n",
    "    for i in range(len(json)):\n",
    "        soup = BeautifulSoup(json[i][key])  # Create the soup object\n",
    "        json[i][key] = re.sub(\"[^a-zA-Z]\", \" \", soup.get_text())  # Clean out the non-alphabetical characters\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Make lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def to_lower(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = json[i][key].lower()\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')  # Remove punctuation, whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = tokenizer.tokenize(json[i][key])\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "# get_tokens(lpt_posts, \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Lemmatize:\n",
    "\n",
    "- This can help with some typos in our word analysis.\n",
    "    - For example, we can use lemmatization to identify `untill`, and make a necessary adjustment to model input\n",
    "- Lemmatization will not be applied to `author`, as these are the usernames attached to the post submission to the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def to_lemma(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = [lemma.lemmatize(j) for j in json[i][key]]\n",
    "    return json\n",
    "#     return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "# to_lemma(lpt_posts[:2], \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Stopwords**\n",
    "\n",
    "- In this iteration of the model, the `LPT` or `lpt` word will be removed from the `title` and `selftext` as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopset.add(\"lpt\")\n",
    "stopset.add(\"ulpt\")\n",
    "\n",
    "# stopset\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopword(json, key):\n",
    "    for i in range(len(json)):\n",
    "        for j in json[i][key]:\n",
    "            if j in stopset:\n",
    "                json[i][key].remove(j)\n",
    "\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5313cedb0105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremove_stopword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlpt_posts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-3d9e2654bc25>\u001b[0m in \u001b[0;36mremove_stopword\u001b[0;34m(json, key)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'remove'"
     ]
    }
   ],
   "source": [
    "remove_stopword(lpt_posts, \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Function to perform each task:\n",
    "\n",
    "- The idea behind this function is to have a function to call on a given feeature that should have all of the preprocessing tasks performed, as listed above.\n",
    "    - Otherwise, each of the above functions can be called on a feature as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def posts_to_words(json, key):\n",
    "\n",
    "    # Remove non-letters:\n",
    "    remove_non_letter(json, key)\n",
    "    \n",
    "    # Make lowercase:\n",
    "    to_lower(json, key)\n",
    "    \n",
    "    # Tokenize:\n",
    "#     get_tokens(json, key)\n",
    "    \n",
    "    # Lemmatize:\n",
    "#     to_lemma(json, key)\n",
    "    \n",
    "    # Remove Stop words:\n",
    "#     remove_stopword(json, key)\n",
    "    \n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Author\n",
    "# lpt_clean_posts = posts_to_words(lpt_posts, \"author\")\n",
    "# ulpt_clean_posts = posts_to_words(ulpt_posts, \"author\")\n",
    "\n",
    "# Title\n",
    "lpt_clean_posts = posts_to_words(lpt_posts, \"title\")\n",
    "ulpt_clean_posts = posts_to_words(ulpt_posts, \"title\")\n",
    "\n",
    "# Selftext\n",
    "# lpt_clean_posts = posts_to_words(lpt_posts, \"selftext\")\n",
    "# ulpt_clean_posts = posts_to_words(ulpt_posts, \"selftext\")\n",
    "\n",
    "# Check\n",
    "# ulpt_clean_posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model Features Set:\n",
    "\n",
    "- ~~`author`~~\n",
    "    - ~~The author of the post~~\n",
    "- `title`\n",
    "    - The title of the post\n",
    "- ~~`selftext`~~\n",
    "    - ~~Included in the post, this is the 'content' of the post and appears under the title.~~\n",
    "    - ~~Not every post in LPT has `selftext` - Many appear with only a title~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(lpt_clean_posts)\n",
    "df2 = pd.DataFrame(ulpt_clean_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 70)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 70 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   all_awardings                  1000 non-null   object \n",
      " 1   allow_live_comments            1000 non-null   bool   \n",
      " 2   author                         1000 non-null   object \n",
      " 3   author_flair_css_class         0 non-null      object \n",
      " 4   author_flair_richtext          897 non-null    object \n",
      " 5   author_flair_text              0 non-null      object \n",
      " 6   author_flair_type              897 non-null    object \n",
      " 7   author_fullname                897 non-null    object \n",
      " 8   author_patreon_flair           897 non-null    object \n",
      " 9   author_premium                 897 non-null    object \n",
      " 10  awarders                       1000 non-null   object \n",
      " 11  can_mod_post                   1000 non-null   bool   \n",
      " 12  contest_mode                   1000 non-null   bool   \n",
      " 13  created_utc                    1000 non-null   int64  \n",
      " 14  domain                         1000 non-null   object \n",
      " 15  full_link                      1000 non-null   object \n",
      " 16  gildings                       1000 non-null   object \n",
      " 17  id                             1000 non-null   object \n",
      " 18  is_crosspostable               1000 non-null   bool   \n",
      " 19  is_meta                        1000 non-null   bool   \n",
      " 20  is_original_content            1000 non-null   bool   \n",
      " 21  is_reddit_media_domain         1000 non-null   bool   \n",
      " 22  is_robot_indexable             1000 non-null   bool   \n",
      " 23  is_self                        1000 non-null   bool   \n",
      " 24  is_video                       1000 non-null   bool   \n",
      " 25  link_flair_background_color    1000 non-null   object \n",
      " 26  link_flair_css_class           230 non-null    object \n",
      " 27  link_flair_richtext            1000 non-null   object \n",
      " 28  link_flair_template_id         420 non-null    object \n",
      " 29  link_flair_text                424 non-null    object \n",
      " 30  link_flair_text_color          1000 non-null   object \n",
      " 31  link_flair_type                1000 non-null   object \n",
      " 32  locked                         1000 non-null   bool   \n",
      " 33  media_only                     1000 non-null   bool   \n",
      " 34  no_follow                      1000 non-null   bool   \n",
      " 35  num_comments                   1000 non-null   int64  \n",
      " 36  num_crossposts                 1000 non-null   int64  \n",
      " 37  over_18                        1000 non-null   bool   \n",
      " 38  parent_whitelist_status        1000 non-null   object \n",
      " 39  permalink                      1000 non-null   object \n",
      " 40  pinned                         1000 non-null   bool   \n",
      " 41  pwls                           1000 non-null   int64  \n",
      " 42  retrieved_on                   1000 non-null   int64  \n",
      " 43  score                          1000 non-null   int64  \n",
      " 44  selftext                       922 non-null    object \n",
      " 45  send_replies                   1000 non-null   bool   \n",
      " 46  spoiler                        1000 non-null   bool   \n",
      " 47  stickied                       1000 non-null   bool   \n",
      " 48  subreddit                      1000 non-null   object \n",
      " 49  subreddit_id                   1000 non-null   object \n",
      " 50  subreddit_subscribers          1000 non-null   int64  \n",
      " 51  subreddit_type                 1000 non-null   object \n",
      " 52  suggested_sort                 500 non-null    object \n",
      " 53  thumbnail                      1000 non-null   object \n",
      " 54  title                          1000 non-null   object \n",
      " 55  total_awards_received          1000 non-null   int64  \n",
      " 56  treatment_tags                 1000 non-null   object \n",
      " 57  url                            1000 non-null   object \n",
      " 58  whitelist_status               1000 non-null   object \n",
      " 59  wls                            1000 non-null   int64  \n",
      " 60  removed_by_category            629 non-null    object \n",
      " 61  post_hint                      48 non-null     object \n",
      " 62  preview                        48 non-null     object \n",
      " 63  author_flair_background_color  103 non-null    object \n",
      " 64  author_flair_text_color        103 non-null    object \n",
      " 65  banned_by                      78 non-null     object \n",
      " 66  edited                         23 non-null     float64\n",
      " 67  author_cakeday                 3 non-null      object \n",
      " 68  crosspost_parent               1 non-null      object \n",
      " 69  crosspost_parent_list          1 non-null      object \n",
      "dtypes: bool(18), float64(1), int64(9), object(42)\n",
      "memory usage: 424.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    lpt  if you are a forgetful person and need to...\n",
       "1    if your friend comes at you with some bullshit...\n",
       "2    lpt  use a paper towel when turning on your si...\n",
       "3    lpt  want a really deep sleep  workout before ...\n",
       "4                                        tip for life \n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove Stopwords\n",
    "\n",
    "- This section was performed here, to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Binarize target `y` variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LifeProTips             500\n",
       "UnethicalLifeProTips    500\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create numeric values for y var to be passed into model\n",
    "\n",
    "df[\"subreddit\"] = df[\"subreddit\"].map({\"LifeProTips\": 1,\n",
    "                                       \"UnethicalLifeProTips\": 0\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/df_model_1.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3054fca1e4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./datasets/df_model_1.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     def to_clipboard(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/df_model_1.0'"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"./datasets/df_model_1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyparams set to lesson defaults\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = cvec.fit_transform(X_train)\n",
    "X_test_sc = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (670, 2711)\n",
      "X_test_sc shape: (330, 2711)\n",
      "\n",
      "X_train_sc feature names: ['abandoned', 'book', 'contactless', 'economy']\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train_sc.shape}\")\n",
    "print(f\"X_test_sc shape: {X_test_sc.shape}\\n\")\n",
    "print(f\"X_train_sc feature names: {cvec.get_feature_names()[0:1000:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    165\n",
       "0    165\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline score:\n",
    "y_test.value_counts()  # even 50/50 split - may need to tweak this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.9151515151515152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train score: {logreg.score(X_train_sc, y_train)}\") \n",
    "print(f\"Test score: {logreg.score(X_test_sc, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model Score Notes:\n",
    "\n",
    "- There is evidence of overfitting. On our train dataset, we scored a perfect **1.0** in accuracy. On the test dataset, the scored was only **0.90**. \n",
    "    - One reason for this was that no stopwords were removed from the raw data.\n",
    "    - On many `title`s, there is a **lpt** or **ulpt** added in the text. This would correlate STRONGLY to which subreddit the text belongs to, and is most likely throwing off our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
