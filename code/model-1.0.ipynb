{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "### Can a Classification model, trained on NLP data, correctly predict subreddit of origin between a 'good advice' and a 'bad advice' subreddit when fed `author`, `title`, and `selftext`?\n",
    "\n",
    "\n",
    "### Notes on the Data and Subreddits:\n",
    "\n",
    "**LifeProTips: (LPT)**\n",
    "- \"Tips that improve your life in one way or another\"\n",
    "- A subreddit dedicated to sharing 'helpful' user-provided advice for navigating a plethora of sitautions.\n",
    "\n",
    "**UnethicalLifeProTips: (ULPT)**\n",
    "- \"An Unethical Life Pro Tip (or ULPT) is a tip that improves your life in a meaningful way, perhaps at the    \n",
    "expense of others and/or with questionable legality. Due to their nature, do not actually follow any of these \n",
    "tipsâ€“they're just for fun. Share your best tips you've picked up throughout your life, and learn from others!\"\n",
    "- A subreddit dedicated to sharing mocking, 'joke' user-provided 'advice on a number of subjects and situations\n",
    "    \n",
    "### Predictors and Target Variable:\n",
    "\n",
    "**Model 1.0:**\n",
    "- The predictor variable is `title`.\n",
    "- The target variable is `subreddit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to call in Reddit info via API\n",
    "\n",
    "**Below: This function:**\n",
    "\n",
    "1) Variabilizes the 'base' url of the Pushshift API\n",
    "\n",
    "2) Uses two editable dictionaries of parameters to add-on to the API URL\n",
    "    - Current parameters are `subreddit` and `size`.\n",
    "\n",
    "3) Assign the `HTTP Reponse` request and variabilize the `status code`.\n",
    "\n",
    "4) An `if` statement checks that the value of the `status_code` for both subreddits are within the bounds of `Success 2XX`.\n",
    "\n",
    "5) If successful:\n",
    "    - The `JSON` is read-in for each subreddit, \n",
    "    - The posts are saved from each JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_posts(subreddit_str, size):\n",
    "    \n",
    "    # Setup URL of API\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"    \n",
    "    \n",
    "    # Create the params of the API URL\n",
    "    params = {\n",
    "        \"subreddit\": subreddit_str,\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    # Response\n",
    "    res = requests.get(base_url, params)\n",
    "    res_check = res.status_code\n",
    "    \n",
    "    # Check response is good\n",
    "    if (res_check >= 200 and res_check < 300):\n",
    "        \n",
    "        # Create JSON:\n",
    "        data = res.json()\n",
    "        posts = data[\"data\"]\n",
    "        \n",
    "        return posts\n",
    "    else:\n",
    "        return f\"Check HTTP Error: {res_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpt_posts = generate_json_posts(\"LifeProTips\", 500)\n",
    "ulpt_posts = generate_json_posts(\"UnethicalLifeProTips\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Posts dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "\n",
      "Each post type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type: {type(lpt_posts)}\\n\")\n",
    "print(f\"Each post type: {type(lpt_posts[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "\n",
    "- **HTML Artifacts:**\n",
    "- **Non-Letters**\n",
    "- **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Non-Letters**\n",
    "\n",
    "The function below takes in the json dictionary object and a specific key to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_letter(json, key):\n",
    "    for i in range(len(json)):\n",
    "        soup = BeautifulSoup(json[i][key])  # Create the soup object\n",
    "        json[i][key] = re.sub(\"[^a-zA-Z]\", \" \", soup.get_text())  # Clean out the non-alphabetical characters\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = json[i][key].lower()\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')  # Remove punctuation, whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = tokenizer.tokenize(json[i][key])\n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "# get_tokens(lpt_posts, \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize:\n",
    "\n",
    "- This can help with some typos in our word analysis.\n",
    "    - For example, we can use lemmatization to identify `untill`, and make a necessary adjustment to model input\n",
    "- Lemmatization will not be applied to `author`, as these are the usernames attached to the post submission to the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lemma(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = [lemma.lemmatize(j) for j in json[i][key]]\n",
    "    return json\n",
    "#     return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "# to_lemma(lpt_posts[:2], \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords**\n",
    "\n",
    "- In the first iteration of this model, the `LPT` or `lpt` word will be removed from the `title` and `selftext` as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopset.add(\"LPT\")  # The capital is technically in alpha order\n",
    "stopset.add(\"lpt\")\n",
    "stopset.add(\"ULPT\")\n",
    "stopset.add(\"ulpt\")\n",
    "\n",
    "# stopset\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(json, key):\n",
    "    for i in range(len(json)):\n",
    "        for j in json[i][key]:\n",
    "            if j in stopset:\n",
    "                json[i][key].remove(j)\n",
    "\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform each task:\n",
    "\n",
    "- The idea behind this function is to have a function to call on a given feeature that should have all of the preprocessing tasks performed, as listed above.\n",
    "    - Otherwise, each of the above functions can be called on a feature as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_to_words(json, key):\n",
    "\n",
    "    # Remove non-letters:\n",
    "    remove_non_letter(json, key)\n",
    "    \n",
    "    # Make lowercase:\n",
    "    to_lower(json, key)\n",
    "    \n",
    "    # Tokenize:\n",
    "#     get_tokens(json, key)\n",
    "    \n",
    "    # Lemmatize:\n",
    "#     to_lemma(json, key)\n",
    "    \n",
    "    # Remove Stop words:\n",
    "#     remove_stopword(json, key)\n",
    "    \n",
    "    return json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author\n",
    "# lpt_clean_posts = posts_to_words(lpt_posts, \"author\")\n",
    "# ulpt_clean_posts = posts_to_words(ulpt_posts, \"author\")\n",
    "\n",
    "# Title\n",
    "lpt_clean_posts = posts_to_words(lpt_posts, \"title\")\n",
    "ulpt_clean_posts = posts_to_words(ulpt_posts, \"title\")\n",
    "\n",
    "# Selftext\n",
    "# lpt_clean_posts = posts_to_words(lpt_posts, \"selftext\")\n",
    "# ulpt_clean_posts = posts_to_words(ulpt_posts, \"selftext\")\n",
    "\n",
    "# Check\n",
    "# ulpt_clean_posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features Set:\n",
    "\n",
    "- ~~`author`~~\n",
    "    - ~~The author of the post~~\n",
    "- `title`\n",
    "    - The title of the post\n",
    "- ~~`selftext`~~\n",
    "    - ~~Included in the post, this is the 'content' of the post and appears under the title.~~\n",
    "    - ~~Not every post in LPT has `selftext` - Many appear with only a title~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(lpt_clean_posts)\n",
    "df2 = pd.DataFrame(ulpt_clean_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 70)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 70 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   all_awardings                  1000 non-null   object \n",
      " 1   allow_live_comments            1000 non-null   bool   \n",
      " 2   author                         1000 non-null   object \n",
      " 3   author_flair_css_class         0 non-null      object \n",
      " 4   author_flair_richtext          884 non-null    object \n",
      " 5   author_flair_text              0 non-null      object \n",
      " 6   author_flair_type              884 non-null    object \n",
      " 7   author_fullname                884 non-null    object \n",
      " 8   author_patreon_flair           884 non-null    object \n",
      " 9   author_premium                 884 non-null    object \n",
      " 10  awarders                       1000 non-null   object \n",
      " 11  can_mod_post                   1000 non-null   bool   \n",
      " 12  contest_mode                   1000 non-null   bool   \n",
      " 13  created_utc                    1000 non-null   int64  \n",
      " 14  domain                         1000 non-null   object \n",
      " 15  full_link                      1000 non-null   object \n",
      " 16  gildings                       1000 non-null   object \n",
      " 17  id                             1000 non-null   object \n",
      " 18  is_crosspostable               1000 non-null   bool   \n",
      " 19  is_meta                        1000 non-null   bool   \n",
      " 20  is_original_content            1000 non-null   bool   \n",
      " 21  is_reddit_media_domain         1000 non-null   bool   \n",
      " 22  is_robot_indexable             1000 non-null   bool   \n",
      " 23  is_self                        1000 non-null   bool   \n",
      " 24  is_video                       1000 non-null   bool   \n",
      " 25  link_flair_background_color    1000 non-null   object \n",
      " 26  link_flair_css_class           241 non-null    object \n",
      " 27  link_flair_richtext            1000 non-null   object \n",
      " 28  link_flair_template_id         427 non-null    object \n",
      " 29  link_flair_text                432 non-null    object \n",
      " 30  link_flair_text_color          1000 non-null   object \n",
      " 31  link_flair_type                1000 non-null   object \n",
      " 32  locked                         1000 non-null   bool   \n",
      " 33  media_only                     1000 non-null   bool   \n",
      " 34  no_follow                      1000 non-null   bool   \n",
      " 35  num_comments                   1000 non-null   int64  \n",
      " 36  num_crossposts                 1000 non-null   int64  \n",
      " 37  over_18                        1000 non-null   bool   \n",
      " 38  parent_whitelist_status        1000 non-null   object \n",
      " 39  permalink                      1000 non-null   object \n",
      " 40  pinned                         1000 non-null   bool   \n",
      " 41  pwls                           1000 non-null   int64  \n",
      " 42  retrieved_on                   1000 non-null   int64  \n",
      " 43  score                          1000 non-null   int64  \n",
      " 44  selftext                       911 non-null    object \n",
      " 45  send_replies                   1000 non-null   bool   \n",
      " 46  spoiler                        1000 non-null   bool   \n",
      " 47  stickied                       1000 non-null   bool   \n",
      " 48  subreddit                      1000 non-null   object \n",
      " 49  subreddit_id                   1000 non-null   object \n",
      " 50  subreddit_subscribers          1000 non-null   int64  \n",
      " 51  subreddit_type                 1000 non-null   object \n",
      " 52  suggested_sort                 500 non-null    object \n",
      " 53  thumbnail                      1000 non-null   object \n",
      " 54  title                          1000 non-null   object \n",
      " 55  total_awards_received          1000 non-null   int64  \n",
      " 56  treatment_tags                 1000 non-null   object \n",
      " 57  url                            1000 non-null   object \n",
      " 58  whitelist_status               1000 non-null   object \n",
      " 59  wls                            1000 non-null   int64  \n",
      " 60  removed_by_category            640 non-null    object \n",
      " 61  post_hint                      43 non-null     object \n",
      " 62  preview                        43 non-null     object \n",
      " 63  author_flair_background_color  116 non-null    object \n",
      " 64  author_flair_text_color        116 non-null    object \n",
      " 65  banned_by                      89 non-null     object \n",
      " 66  edited                         27 non-null     float64\n",
      " 67  author_cakeday                 4 non-null      object \n",
      " 68  crosspost_parent               1 non-null      object \n",
      " 69  crosspost_parent_list          1 non-null      object \n",
      "dtypes: bool(18), float64(1), int64(9), object(42)\n",
      "memory usage: 424.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    lpt  whenever you say  sorry   you should inst...\n",
       "1    lpt   use a fake number app when looking onlin...\n",
       "2    lpt  at the end of the quarantine and after at...\n",
       "3    what to do if your wallet is lost or stolen   lpt\n",
       "4    lpt  if you decide to start walking due to cor...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarize target `y` variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnethicalLifeProTips    500\n",
       "LifeProTips             500\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric values for y var to be passed into model\n",
    "\n",
    "df[\"subreddit\"] = df[\"subreddit\"].map({\"LifeProTips\": 1,\n",
    "                                       \"UnethicalLifeProTips\": 0\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./datasets/df_model_1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyparams set to lesson defaults\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = cvec.fit_transform(X_train)\n",
    "X_test_sc = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (670, 2754)\n",
      "X_test_sc shape: (330, 2754)\n",
      "\n",
      "X_train_sc feature names: ['ability', 'booze', 'contact', 'earth']\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train_sc.shape}\")\n",
    "print(f\"X_test_sc shape: {X_test_sc.shape}\\n\")\n",
    "print(f\"X_train_sc feature names: {cvec.get_feature_names()[0:1000:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    165\n",
       "0    165\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline score:\n",
    "y_test.value_counts()  # even 50/50 split - may need to tweak this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 1.0\n",
      "Test score: 0.906060606060606\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train score: {logreg.score(X_train_sc, y_train)}\") \n",
    "print(f\"Test score: {logreg.score(X_test_sc, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model Score Notes:\n",
    "\n",
    "- There is evidence of overfitting. On our train dataset, we scored a perfect **1.0** in accuracy. On the test dataset, the scored was only **0.90**. \n",
    "    - One reason for this was that no stopwords were removed from the raw data.\n",
    "    - On many `title`s, there is a **lpt** or **ulpt** added in the text. This would correlate STRONGLY to which subreddit the text belongs to, and is most likely throwing off our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
