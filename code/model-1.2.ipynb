{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "### Can the text of a subreddit post's `title` and `selftext` reliably predict if a post is 'good advice' or 'bad advice'?\n",
    "\n",
    "    \n",
    "    \n",
    "### Predictors and Target Variable:\n",
    "\n",
    "**Model 1.2:**\n",
    "- The predictor variable is `title`.\n",
    "- The target variable is `subreddit`.\n",
    "\n",
    "### Pipeline & GridSearch:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create JSON Files via API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_posts(subreddit_str, size):\n",
    "    \n",
    "    # Setup URL of API\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"    \n",
    "    \n",
    "    # Create the params of the API URL\n",
    "    params = {\n",
    "        \"subreddit\": subreddit_str,\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    # Response\n",
    "    res = requests.get(base_url, params)\n",
    "    res_check = res.status_code\n",
    "    \n",
    "    # Check response is good\n",
    "    if (res_check >= 200 and res_check < 300):\n",
    "        \n",
    "        # Create JSON:\n",
    "        data = res.json()\n",
    "        posts = data[\"data\"]\n",
    "        \n",
    "        return posts\n",
    "    else:\n",
    "        return f\"Check HTTP Error: {res_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpt_posts = generate_json_posts(\"LifeProTips\", 500)\n",
    "ulpt_posts = generate_json_posts(\"UnethicalLifeProTips\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(lpt_posts, \"../datasets/lpt_posts_json\")\n",
    "pd.to_pickle(ulpt_posts, \"../datasets/ulpt_posts_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features Set:\n",
    "\n",
    "- ~~`author`~~\n",
    "    - ~~The author of the post~~\n",
    "- `title`\n",
    "    - The title of the post\n",
    "- `selftext`\n",
    "    - Included in the post, this is the 'content' of the post and appears under the title.\n",
    "    - Not every post in LPT has `selftext` - Many appear with only a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_df = pd.DataFrame(lpt_posts)\n",
    "ulpt_df = pd.DataFrame(ulpt_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lpt_df.append(ulpt_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 70)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 70 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   all_awardings                  1000 non-null   object \n",
      " 1   allow_live_comments            1000 non-null   bool   \n",
      " 2   author                         1000 non-null   object \n",
      " 3   author_flair_css_class         0 non-null      object \n",
      " 4   author_flair_richtext          901 non-null    object \n",
      " 5   author_flair_text              0 non-null      object \n",
      " 6   author_flair_type              901 non-null    object \n",
      " 7   author_fullname                901 non-null    object \n",
      " 8   author_patreon_flair           901 non-null    object \n",
      " 9   author_premium                 901 non-null    object \n",
      " 10  awarders                       1000 non-null   object \n",
      " 11  can_mod_post                   1000 non-null   bool   \n",
      " 12  contest_mode                   1000 non-null   bool   \n",
      " 13  created_utc                    1000 non-null   int64  \n",
      " 14  domain                         1000 non-null   object \n",
      " 15  full_link                      1000 non-null   object \n",
      " 16  gildings                       1000 non-null   object \n",
      " 17  id                             1000 non-null   object \n",
      " 18  is_crosspostable               1000 non-null   bool   \n",
      " 19  is_meta                        1000 non-null   bool   \n",
      " 20  is_original_content            1000 non-null   bool   \n",
      " 21  is_reddit_media_domain         1000 non-null   bool   \n",
      " 22  is_robot_indexable             1000 non-null   bool   \n",
      " 23  is_self                        1000 non-null   bool   \n",
      " 24  is_video                       1000 non-null   bool   \n",
      " 25  link_flair_background_color    1000 non-null   object \n",
      " 26  link_flair_css_class           226 non-null    object \n",
      " 27  link_flair_richtext            1000 non-null   object \n",
      " 28  link_flair_template_id         415 non-null    object \n",
      " 29  link_flair_text                420 non-null    object \n",
      " 30  link_flair_text_color          1000 non-null   object \n",
      " 31  link_flair_type                1000 non-null   object \n",
      " 32  locked                         1000 non-null   bool   \n",
      " 33  media_only                     1000 non-null   bool   \n",
      " 34  no_follow                      1000 non-null   bool   \n",
      " 35  num_comments                   1000 non-null   int64  \n",
      " 36  num_crossposts                 1000 non-null   int64  \n",
      " 37  over_18                        1000 non-null   bool   \n",
      " 38  parent_whitelist_status        1000 non-null   object \n",
      " 39  permalink                      1000 non-null   object \n",
      " 40  pinned                         1000 non-null   bool   \n",
      " 41  pwls                           1000 non-null   int64  \n",
      " 42  removed_by_category            634 non-null    object \n",
      " 43  retrieved_on                   1000 non-null   int64  \n",
      " 44  score                          1000 non-null   int64  \n",
      " 45  selftext                       926 non-null    object \n",
      " 46  send_replies                   1000 non-null   bool   \n",
      " 47  spoiler                        1000 non-null   bool   \n",
      " 48  stickied                       1000 non-null   bool   \n",
      " 49  subreddit                      1000 non-null   object \n",
      " 50  subreddit_id                   1000 non-null   object \n",
      " 51  subreddit_subscribers          1000 non-null   int64  \n",
      " 52  subreddit_type                 1000 non-null   object \n",
      " 53  suggested_sort                 500 non-null    object \n",
      " 54  thumbnail                      1000 non-null   object \n",
      " 55  title                          1000 non-null   object \n",
      " 56  total_awards_received          1000 non-null   int64  \n",
      " 57  treatment_tags                 1000 non-null   object \n",
      " 58  url                            1000 non-null   object \n",
      " 59  whitelist_status               1000 non-null   object \n",
      " 60  wls                            1000 non-null   int64  \n",
      " 61  post_hint                      48 non-null     object \n",
      " 62  preview                        48 non-null     object \n",
      " 63  author_flair_background_color  99 non-null     object \n",
      " 64  author_flair_text_color        99 non-null     object \n",
      " 65  banned_by                      74 non-null     object \n",
      " 66  edited                         21 non-null     float64\n",
      " 67  author_cakeday                 1 non-null      object \n",
      " 68  crosspost_parent               1 non-null      object \n",
      " 69  crosspost_parent_list          1 non-null      object \n",
      "dtypes: bool(18), float64(1), int64(9), object(42)\n",
      "memory usage: 424.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  [REQUEST] Teen love\n",
       "1     20 páginas para encontrar ofertas de teletrabajo\n",
       "2    LPT: Get a bidet. They will drastically reduce...\n",
       "3    LPT: If you want to stay close to friends and ...\n",
       "4    LPT: When halfway through a sleeve of cookies,...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "\n",
    "- **HTML Artifacts:**\n",
    "- **Non-Letters**\n",
    "- **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize:**\n",
    "\n",
    "- This can help with some typos in our word analysis.\n",
    "    - For example, we can use lemmatization to identify `untill`, and make a necessary adjustment to model input\n",
    "- Lemmatization will not be applied to `author`, as these are the usernames attached to the post submission to the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lemma(json, key):\n",
    "    for i in range(len(json)):\n",
    "        json[i][key] = [lemma.lemmatize(j) for j in json[i][key]]\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = [\" \".join([i for i in x.split()\n",
    "                         if i not in stopset])\n",
    "                         for x in df[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     20 páginas para encontrar ofertas de teletrabajo\n",
       "1    LPT: Get bidet. They drastically reduce amount...\n",
       "2    LPT: If want stay close friends family, respon...\n",
       "3    LPT: When halfway sleeve cookies, pull sleeve ...\n",
       "4                                 Happy 4/20 day enjoy\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stopwords\n",
    "- In this iteration of the model, the `LPT` or `lpt` word will be removed from the `title` and `selftext` as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopset.add(\"lpt\")\n",
    "stopset.add(\"lptrequest\")\n",
    "\n",
    "stopset.add(\"ulpt\")\n",
    "stopset.add(\"ulptrequest\")\n",
    "\n",
    "# stopset\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform each Preprocessing task:\n",
    "\n",
    "- The idea behind this function is to have a function to call on a given feeature that should have all of the preprocessing tasks performed, as listed above.\n",
    "    - Otherwise, each of the above functions can be called on a feature as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_df(data, col):\n",
    "\n",
    "    # Remove non-letters:\n",
    "    new_lst = []\n",
    "    for i in data[col]:\n",
    "        soup = BeautifulSoup(i, \"lxml\")\n",
    "        new_lst.append(re.sub(\"[^a-zA-Z]\", \" \", soup.get_text()))\n",
    "    data[col] = new_lst\n",
    "    # Some reference to: https://www.reddit.com/r/learnpython/comments/an62wx/how_to_remove_html_from_pandas_dataframe_without/\n",
    "    \n",
    "    \n",
    "    # Make lowercase:\n",
    "    data[col] = data[col].str.lower()\n",
    "    \n",
    "    # THIS IS WHERE I LEFT OFF!!!\n",
    "    # Lemmatize:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    print(lemma.lemmatize(\"reference\"))\n",
    "    \n",
    "    for row in data[col]:\n",
    "        new_lst = [lemma.lemmatize(row)]\n",
    "    print(new_lst)\n",
    "    # Remove Stopwords:\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference\n",
      "['ulpt request  as a new grad  i want to stand a chance in this economy  is a fake reference out of the question ']\n"
     ]
    }
   ],
   "source": [
    "clean_df(df, \"title\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ofertas'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"ofertas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   request  teen love\n",
       "1        p ginas para encontrar ofertas de teletrabajo\n",
       "2    lpt  get a bidet  they will drastically reduce...\n",
       "3    lpt  if you want to stay close to friends and ...\n",
       "4    lpt  when halfway through a sleeve of cookies ...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarize target `y` variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric values for y var to be passed into model\n",
    "\n",
    "df[\"subreddit\"] = df[\"subreddit\"].map({\"LifeProTips\": 1,\n",
    "                                       \"UnethicalLifeProTips\": 0\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./datasets/df_model_1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyparams set to lesson defaults\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = cvec.fit_transform(X_train)\n",
    "X_test_sc = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train_sc.shape}\")\n",
    "print(f\"X_test_sc shape: {X_test_sc.shape}\\n\")\n",
    "print(f\"X_train_sc feature names: {cvec.get_feature_names()[0:1000:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline score:\n",
    "y_test.value_counts()  # even 50/50 split - may need to tweak this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train score: {}\")\n",
    "# print(f\"Test score: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model Score Notes:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
