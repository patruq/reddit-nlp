{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "### Can the text of a subreddit post's `title` and `selftext` reliably predict if a post is 'good advice' or 'bad advice'?\n",
    "\n",
    "    \n",
    "    \n",
    "### Predictors and Target Variable:\n",
    "\n",
    "**Model 1.2:**\n",
    "- The predictor variable is `title`.\n",
    "- The target variable is `subreddit`.\n",
    "\n",
    "### Pipeline & GridSearch:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create JSON Files via API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_posts(subreddit_str, size):\n",
    "    \n",
    "    # Setup URL of API\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"    \n",
    "    \n",
    "    # Create the params of the API URL\n",
    "    params = {\n",
    "        \"subreddit\": subreddit_str,\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    # Response\n",
    "    res = requests.get(base_url, params)\n",
    "    res_check = res.status_code\n",
    "    \n",
    "    # Check response is good\n",
    "    if (res_check >= 200 and res_check < 300):\n",
    "        \n",
    "        # Create JSON:\n",
    "        data = res.json()\n",
    "        posts = data[\"data\"]\n",
    "        \n",
    "        return posts\n",
    "    else:\n",
    "        return f\"Check HTTP Error: {res_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpt_posts = generate_json_posts(\"LifeProTips\", 500)\n",
    "ulpt_posts = generate_json_posts(\"UnethicalLifeProTips\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save JSON Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(lpt_posts, \"../datasets/lpt_posts_json\")\n",
    "pd.to_pickle(ulpt_posts, \"../datasets/ulpt_posts_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features Set:\n",
    "\n",
    "- ~~`author`~~\n",
    "    - ~~The author of the post~~\n",
    "- `title`\n",
    "    - The title of the post\n",
    "- `selftext`\n",
    "    - Included in the post, this is the 'content' of the post and appears under the title.\n",
    "    - Not every post in LPT has `selftext` - Many appear with only a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_df = pd.DataFrame(lpt_posts)\n",
    "ulpt_df = pd.DataFrame(ulpt_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lpt_df.append(ulpt_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 70)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 70 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   all_awardings                  1000 non-null   object \n",
      " 1   allow_live_comments            1000 non-null   bool   \n",
      " 2   author                         1000 non-null   object \n",
      " 3   author_flair_css_class         0 non-null      object \n",
      " 4   author_flair_richtext          896 non-null    object \n",
      " 5   author_flair_text              0 non-null      object \n",
      " 6   author_flair_type              896 non-null    object \n",
      " 7   author_fullname                896 non-null    object \n",
      " 8   author_patreon_flair           896 non-null    object \n",
      " 9   author_premium                 896 non-null    object \n",
      " 10  awarders                       1000 non-null   object \n",
      " 11  can_mod_post                   1000 non-null   bool   \n",
      " 12  contest_mode                   1000 non-null   bool   \n",
      " 13  created_utc                    1000 non-null   int64  \n",
      " 14  domain                         1000 non-null   object \n",
      " 15  full_link                      1000 non-null   object \n",
      " 16  gildings                       1000 non-null   object \n",
      " 17  id                             1000 non-null   object \n",
      " 18  is_crosspostable               1000 non-null   bool   \n",
      " 19  is_meta                        1000 non-null   bool   \n",
      " 20  is_original_content            1000 non-null   bool   \n",
      " 21  is_reddit_media_domain         1000 non-null   bool   \n",
      " 22  is_robot_indexable             1000 non-null   bool   \n",
      " 23  is_self                        1000 non-null   bool   \n",
      " 24  is_video                       1000 non-null   bool   \n",
      " 25  link_flair_background_color    1000 non-null   object \n",
      " 26  link_flair_richtext            1000 non-null   object \n",
      " 27  link_flair_text_color          1000 non-null   object \n",
      " 28  link_flair_type                1000 non-null   object \n",
      " 29  locked                         1000 non-null   bool   \n",
      " 30  media_only                     1000 non-null   bool   \n",
      " 31  no_follow                      1000 non-null   bool   \n",
      " 32  num_comments                   1000 non-null   int64  \n",
      " 33  num_crossposts                 1000 non-null   int64  \n",
      " 34  over_18                        1000 non-null   bool   \n",
      " 35  parent_whitelist_status        1000 non-null   object \n",
      " 36  permalink                      1000 non-null   object \n",
      " 37  pinned                         1000 non-null   bool   \n",
      " 38  pwls                           1000 non-null   int64  \n",
      " 39  removed_by_category            624 non-null    object \n",
      " 40  retrieved_on                   1000 non-null   int64  \n",
      " 41  score                          1000 non-null   int64  \n",
      " 42  selftext                       924 non-null    object \n",
      " 43  send_replies                   1000 non-null   bool   \n",
      " 44  spoiler                        1000 non-null   bool   \n",
      " 45  stickied                       1000 non-null   bool   \n",
      " 46  subreddit                      1000 non-null   object \n",
      " 47  subreddit_id                   1000 non-null   object \n",
      " 48  subreddit_subscribers          1000 non-null   int64  \n",
      " 49  subreddit_type                 1000 non-null   object \n",
      " 50  suggested_sort                 500 non-null    object \n",
      " 51  thumbnail                      1000 non-null   object \n",
      " 52  title                          1000 non-null   object \n",
      " 53  total_awards_received          1000 non-null   int64  \n",
      " 54  treatment_tags                 1000 non-null   object \n",
      " 55  url                            1000 non-null   object \n",
      " 56  whitelist_status               1000 non-null   object \n",
      " 57  wls                            1000 non-null   int64  \n",
      " 58  link_flair_css_class           244 non-null    object \n",
      " 59  link_flair_template_id         436 non-null    object \n",
      " 60  link_flair_text                439 non-null    object \n",
      " 61  author_flair_background_color  104 non-null    object \n",
      " 62  author_flair_text_color        104 non-null    object \n",
      " 63  banned_by                      76 non-null     object \n",
      " 64  post_hint                      45 non-null     object \n",
      " 65  preview                        45 non-null     object \n",
      " 66  author_cakeday                 1 non-null      object \n",
      " 67  edited                         20 non-null     float64\n",
      " 68  crosspost_parent               1 non-null      object \n",
      " 69  crosspost_parent_list          1 non-null      object \n",
      "dtypes: bool(18), float64(1), int64(9), object(42)\n",
      "memory usage: 424.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     How can we help our communities during COVID-19.\n",
       "1    LPT Request: When you feel physically tired af...\n",
       "2    When you feel physically tired after a long da...\n",
       "3    If your bank has been charging you fees for a ...\n",
       "4    LPT: Have a professional voice actor voice you...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "\n",
    "- **HTML Artifacts:**\n",
    "- **Non-Letters**\n",
    "- **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatize:**\n",
    "\n",
    "- This can help with some typos in our word analysis.\n",
    "    - For example, we can use lemmatization to identify `untill`, and make a necessary adjustment to model input\n",
    "- Lemmatization will not be applied to `author`, as these are the usernames attached to the post submission to the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lemma(data, col):\n",
    "#     data[col] = [i.split() for i in data[col]]  # Creates a list of split words\n",
    "    \n",
    "    new_lst = []\n",
    "    \n",
    "    data[col].apply(lambda i: lemma.lemmatize(i))\n",
    "    return data[col]\n",
    "#     for i in range(len(data[col])):\n",
    "# #         print(data[col][i])  # Each list of words in a post\n",
    "        \n",
    "#         for j in data[col][i]:\n",
    "# #             print(j)  # Each word\n",
    "#             new_lst.append(lemma.lemmatize(j))\n",
    "# #             print(j)\n",
    "#             data[col][i] = new_lst\n",
    "#     print(data[col])\n",
    "            \n",
    "    \n",
    "#     return  \" \".join(i for i in data[col])  # This takes the list and joins it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 1 required positional argument: 'j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-01d2ff3fd0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to_lemma(df, \"title\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'j'"
     ]
    }
   ],
   "source": [
    "# to_lemma(df, \"title\")\n",
    "df[\"title\"].apply(lambda i, j: lemma.lemmatize(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       How can we help our communities during COVID-19.\n",
       "1      LPT Request: When you feel physically tired af...\n",
       "2      When you feel physically tired after a long da...\n",
       "3      If your bank has been charging you fees for a ...\n",
       "4      LPT: Have a professional voice actor voice you...\n",
       "                             ...                        \n",
       "995                                         UTLP Request\n",
       "996             ULPT Cheating in my online mockup exams?\n",
       "997    ULPT: Every retailer, essentially, is extendin...\n",
       "998    Every store, essentially, is extending their r...\n",
       "999    ULPT REQUEST: Found credit card after purchasi...\n",
       "Name: title, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title\"] = [\" \".join([i for i in x.split()\n",
    "                         if i not in stopset])\n",
    "                         for x in df[\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     20 p√°ginas para encontrar ofertas de teletrabajo\n",
       "1    LPT: Get bidet. They drastically reduce amount...\n",
       "2    LPT: If want stay close friends family, respon...\n",
       "3    LPT: When halfway sleeve cookies, pull sleeve ...\n",
       "4                                 Happy 4/20 day enjoy\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stopwords\n",
    "- In this iteration of the model, the `LPT` or `lpt` word will be removed from the `title` and `selftext` as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopset.add(\"lpt\")\n",
    "stopset.add(\"lptrequest\")\n",
    "\n",
    "stopset.add(\"ulpt\")\n",
    "stopset.add(\"ulptrequest\")\n",
    "\n",
    "# stopset\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform each Preprocessing task:\n",
    "\n",
    "- The idea behind this function is to have a function to call on a given feeature that should have all of the preprocessing tasks performed, as listed above.\n",
    "    - Otherwise, each of the above functions can be called on a feature as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_df(data, col):\n",
    "\n",
    "    # Remove non-letters:\n",
    "    new_lst = []\n",
    "    for i in data[col]:\n",
    "        soup = BeautifulSoup(i, \"lxml\")\n",
    "        new_lst.append(re.sub(\"[^a-zA-Z]\", \" \", soup.get_text()))\n",
    "    data[col] = new_lst\n",
    "    # Some reference to: https://www.reddit.com/r/learnpython/comments/an62wx/how_to_remove_html_from_pandas_dataframe_without/\n",
    "    \n",
    "    \n",
    "    # Make lowercase:\n",
    "    data[col] = data[col].str.lower()\n",
    "    \n",
    "\n",
    "    # Lemmatize:\n",
    "    lemma = WordNetLemmatizer()\n",
    "    data[col].apply(lambda i: lemma.lemmatizer(i))\n",
    "    \n",
    "#     for row in data[col]:\n",
    "#         new_lst = [lemma.lemmatize(row)]\n",
    "#     print(new_lst)\n",
    "    # Remove Stopwords:\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrickmccaul/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/bs4/__init__.py:389: UserWarning: \"https://www.reddit.com/r/vancouver/comments/g540z0/when_you_see_guide_dogs_make_way_for_them_and/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "clean_df(df, \"title\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     how can we help our communities during covid    \n",
       "1    lpt request  when you feel physically tired af...\n",
       "2    when you feel physically tired after a long da...\n",
       "3    if your bank has been charging you fees for a ...\n",
       "4    lpt  have a professional voice actor voice you...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarize target `y` variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric values for y var to be passed into model\n",
    "\n",
    "df[\"subreddit\"] = df[\"subreddit\"].map({\"LifeProTips\": 1,\n",
    "                                       \"UnethicalLifeProTips\": 0\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"subreddit\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"./datasets/df_model_1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"title\"]\n",
    "y = df[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyparams set to lesson defaults\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = cvec.fit_transform(X_train)\n",
    "X_test_sc = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train_sc.shape}\")\n",
    "print(f\"X_test_sc shape: {X_test_sc.shape}\\n\")\n",
    "print(f\"X_train_sc feature names: {cvec.get_feature_names()[0:1000:250]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline score:\n",
    "y_test.value_counts()  # even 50/50 split - may need to tweak this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Train score: {}\")\n",
    "# print(f\"Test score: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model Score Notes:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
