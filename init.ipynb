{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "\n",
    "### Can a Classification model, trained on NLP data, correctly predict subreddit of origin between a 'good advice' and a 'bad advice' subreddit when fed `author`, `title`, and `selftext`?\n",
    "\n",
    "\n",
    "### Notes on the Data and Subreddits:\n",
    "\n",
    "**LifeProTips: (LPT)**\n",
    "- \"Tips that improve your life in one way or another\"\n",
    "- A subreddit dedicated to sharing 'helpful' user-provided advice for navigating a plethora of sitautions.\n",
    "- ***The number `1` will always represent the LPT subreddit when attached to a variable.***\n",
    "    - For example, `posts_1` or `data_1` refer to the LPT subreddit.\n",
    "\n",
    "**UnethicalLifeProTips: (ULPT)**\n",
    "- \"An Unethical Life Pro Tip (or ULPT) is a tip that improves your life in a meaningful way, perhaps at the    \n",
    "expense of others and/or with questionable legality. Due to their nature, do not actually follow any of these \n",
    "tipsâ€“they're just for fun. Share your best tips you've picked up throughout your life, and learn from others!\"\n",
    "- A subreddit dedicated to sharing mocking, 'joke' user-provided 'advice on a number of subjects and situations\n",
    "-     - ***The number `2` will always represent the ULPT subreddit when attached to a variable.***\n",
    "    - For example, `posts_2` or `data_2` refer to the ULPT subreddit.\n",
    "    \n",
    "### Predictor and Target Variable:\n",
    "\n",
    "**The predictor variables are `author`, `title`, and `selftext`.**\n",
    "\n",
    "**The target variable is `subreddit`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to call in Reddit info via API\n",
    "\n",
    "**Below: This function:**\n",
    "\n",
    "1) Variabilizes the 'base' url of the Pushshift API\n",
    "\n",
    "2) Uses two editable dictionaries of parameters to add-on to the API URL\n",
    "    - Current parameters are `subreddit` and `size`.\n",
    "\n",
    "3) Assign the `HTTP Reponse` request and variabilize the `status code`.\n",
    "\n",
    "4) An `if` statement checks that the value of the `status_code` for both subreddits are within the bounds of `Success 2XX`.\n",
    "\n",
    "5) If successful:\n",
    "    - The `JSON` is read-in for each subreddit, \n",
    "    - The posts are saved from each JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_posts(subreddit_str, size):\n",
    "    \n",
    "    # Setup URL of API\n",
    "    base_url = \"https://api.pushshift.io/reddit/search/submission\"    \n",
    "    \n",
    "    # Create the params of the API URL\n",
    "    params = {\n",
    "        \"subreddit\": subreddit_str,\n",
    "        \"size\": size\n",
    "    }\n",
    "\n",
    "    # Response\n",
    "    res = requests.get(base_url, params)\n",
    "    res_check = res.status_code\n",
    "    \n",
    "    # Check response is good\n",
    "    if (res_check >= 200 and res_check < 300):\n",
    "        \n",
    "        # Create JSON:\n",
    "        data = res.json()\n",
    "        posts = data[\"data\"]\n",
    "        \n",
    "        return posts\n",
    "    else:\n",
    "        return f\"Check HTTP Error: {res_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lpt_posts = generate_json_posts(\"LifeProTips\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Posts dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "\n",
      "Each post type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type: {type(lpt_posts)}\\n\")\n",
    "print(f\"Each post type: {type(lpt_posts[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lists of Text to Manipulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the keys we will want to manipulate:\n",
    "\n",
    "def get_list(posts_list, feature):\n",
    "    lst = []\n",
    "\n",
    "    # Iterate through my list:\n",
    "    for post in posts_list:\n",
    "\n",
    "        # Run through the post dict keys:\n",
    "        for key, value in post.items():\n",
    "\n",
    "            # Get the value into a list\n",
    "            if key == feature:\n",
    "                lst.append(value)\n",
    "                \n",
    "    return lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_authors = get_list(lpt_posts, \"author\")\n",
    "# lpt_authors\n",
    "\n",
    "lpt_titles = get_list(lpt_posts, \"title\")\n",
    "# lpt_titles\n",
    "\n",
    "lpt_selftext = get_list(lpt_posts, \"selftext\")\n",
    "# lpt_selttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning:\n",
    "\n",
    "- **HTML Artifacts:**\n",
    "- **Non-Letters**\n",
    "- **Stopwords**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Non-Letters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_letters(lst):\n",
    "    new_lst = []\n",
    "    for i in lst:\n",
    "        soup = BeautifulSoup(i)\n",
    "        new_lst.append(re.sub(\"[^a-zA-Z]\", \" \", soup.get_text()))\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpt_authors = remove_non_letters(lpt_authors)\n",
    "lpt_titles = remove_non_letters(lpt_titles)\n",
    "lpt_selftext = remove_non_letters(lpt_selftext)\n",
    "# lpt_selftext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords**\n",
    "\n",
    "- In the first iteration of this model, the `LPT` or `lpt` word will be removed as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LPT',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'lpt',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopset = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopset.add(\"LPT\")  # The capital is technically in alpha order\n",
    "stopset.add(\"lpt\")\n",
    "\n",
    "stopset\n",
    "# https://stackoverflow.com/questions/5511708/adding-words-to-nltk-stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')  # Remove punctuation, whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_list(lst):\n",
    "    token_lst = []\n",
    "    for i in lst:\n",
    "        token_lst.append(tokenizer.tokenize(i.lower()))\n",
    "    return token_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_author_tokens = tokenize_list(lpt_authors)\n",
    "lpt_titles_tokens = tokenize_list(lpt_titles)\n",
    "lpt_selftext_tokens = tokenize_list(lpt_selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of lists:\n",
    "\n",
    "# lpt_titles_tokens\n",
    "# lpt_author_tokens\n",
    "# lpt_selftext_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize:\n",
    "\n",
    "- This can help with some typos in our word analysis.\n",
    "    - For example, we can use lemmatization to identify `untill`, and make a necessary adjustment to model input\n",
    "- Lemmatization will not be applied to `author`, as these are the usernames attached to the post submission to the subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(token_list):\n",
    "    \n",
    "    # Instantiate list\n",
    "    tokens_lem = []\n",
    "    \n",
    "    # Iterate through the list of tokenized posts\n",
    "    for lst in token_list:\n",
    "        \n",
    "        # Add each lemmatized word to new list\n",
    "        tokens_lem.append([lemma.lemmatize(i) for i in lst])\n",
    "    \n",
    "    return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_title_lemma = get_lemma(lpt_titles_tokens)\n",
    "# lpt_title_lemma\n",
    "lpt_selftext_lemma = get_lemma(lpt_selftext_tokens)\n",
    "# lpt_selftext_lemme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(lpt_titles_tokens, lpt_title_lemma))  # not sure how helpful this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, j in list(zip(lpt_titles_tokens, lpt_title_lemma)):\n",
    "#     if i != j:\n",
    "#         print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features Set:\n",
    "\n",
    "- `author`\n",
    "    - The author of the post\n",
    "- `title`\n",
    "    - The title of the post\n",
    "- `selftext`\n",
    "    - Included in the post, this is the 'content' of the post and appears under the title.\n",
    "    - Not every post in LPT has `selftext` - Many appear with only a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(post_list):\n",
    "    df = pd.DataFrame(post_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpt_df = create_df(lpt_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_col_info(df, col1, col2, col3):\n",
    "    print(\"NaN\")\n",
    "    print(f\"{col1}:\", df[col1].isna().sum())\n",
    "    print(f\"{col2}:\", df[col2].isna().sum())\n",
    "    print(f\"{col3}:\", df[col3].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN\n",
      "author: 0\n",
      "title: 0\n",
      "selftext: 67\n"
     ]
    }
   ],
   "source": [
    "print_col_info(lpt_df, \"author\", \"title\", \"selftext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: need an expensive prescription drug but h...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: After a shower, dry yourself in there to ...</td>\n",
       "      <td>So, I personally don't know if doing that is j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: After a shower, dry up in there to keep y...</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>If shady people come up and ask you if your na...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>LPT: If youâ€™re going to bring a birthday cake ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title  \\\n",
       "0  LifeProTips  LPT: need an expensive prescription drug but h...   \n",
       "1  LifeProTips  LPT: After a shower, dry yourself in there to ...   \n",
       "2  LifeProTips  LPT: After a shower, dry up in there to keep y...   \n",
       "3  LifeProTips  If shady people come up and ask you if your na...   \n",
       "4  LifeProTips  LPT: If youâ€™re going to bring a birthday cake ...   \n",
       "\n",
       "                                            selftext  \n",
       "0                                                     \n",
       "1  So, I personally don't know if doing that is j...  \n",
       "2                                          [deleted]  \n",
       "3                                                NaN  \n",
       "4                                                     "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"subreddit\", \"title\", \"selftext\"]].head()  # Title has 'LPT' in it, could throw the model off "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Preprocessing:\n",
    "\n",
    "Common column conversions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binarize_bool(posts):\n",
    "#     for post in posts:\n",
    "#         for key in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles(posts_list):\n",
    "    title_list = []\n",
    "    \n",
    "    for post in posts_list:\n",
    "        for key, value in post.items():\n",
    "            if key == \"title\":\n",
    "                title_list.append(value)\n",
    "    return title_list\n",
    "\n",
    "def get_selftext(posts_list):\n",
    "    selftext_list = []\n",
    "    \n",
    "    for post in posts_list:\n",
    "        for key, value in post.items():\n",
    "            if key == \"selftext\":\n",
    "                selftext_list.append(value)\n",
    "    return selftext_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove HTML Artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If youâ€™re living alone, while in quarantine, disable your face id/touch id, it will save you a lot of time'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(lst):\n",
    "    \n",
    "    for i in lst:\n",
    "        soup = BeautifulSoup(i)\n",
    "    return soup.get_text(i)\n",
    "\n",
    "\n",
    "remove_html(lpt_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LPT: If you feel pressured into complementing somebody\\'s baby even though you think it\\'s ugly, say \"Now THAT\\'S a baby!\"'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lpt_titles[50]  # This selftext has HTML artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LPT: If you feel pressured into complementing somebody\\'s baby even though you think it\\'s ugly, say \"Now THAT\\'S a baby!\"'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = BeautifulSoup(lpt_titles[50])\n",
    "ex1.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
